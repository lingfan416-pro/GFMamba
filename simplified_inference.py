#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆé›¶å½±å“æ–¹æ¡ˆï¼šä¸ä¾èµ–mamba_ssm
ä½¿ç”¨æ‚¨çš„MOVå’ŒTXTæ–‡ä»¶è¿›è¡Œæƒ…æ„Ÿåˆ†æ
"""

import os
import pickle
import numpy as np
import torch
import yaml
import librosa
import cv2
import tempfile
import subprocess
from transformers import AutoTokenizer, AutoModel
import warnings
warnings.filterwarnings("ignore")

class SimplifiedProcessor:
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–BERT
        try:
            self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
            self.text_encoder = AutoModel.from_pretrained('bert-base-uncased').to(self.device)
            print("âœ… BERTæ–‡æœ¬ç¼–ç å™¨åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ BERTç¼–ç å™¨åŠ è½½å¤±è´¥: {e}")
            self.tokenizer = None
            self.text_encoder = None
    
    def process_text_file(self, txt_path):
        """å¤„ç†TXTæ–‡ä»¶"""
        print(f"ğŸ“ å¤„ç†æ–‡æœ¬æ–‡ä»¶: {txt_path}")
        
        with open(txt_path, 'r', encoding='utf-8') as f:
            text = f.read().strip()
        
        print(f"ğŸ“– æ–‡æœ¬å†…å®¹: {text[:100]}...")
        print(f"ğŸ“ æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
        
        if self.tokenizer is None:
            print("âš ï¸ ä½¿ç”¨æ ‡å‡†åˆ†å¸ƒç”Ÿæˆæ–‡æœ¬ç‰¹å¾")
            return np.random.normal(0, 1, (50, 768)).astype(np.float32)
        
        # ä½¿ç”¨BERTç¼–ç 
        inputs = self.tokenizer(text, return_tensors='pt', padding=True, 
                              truncation=True, max_length=512).to(self.device)
        
        with torch.no_grad():
            outputs = self.text_encoder(**inputs)
            features = outputs.last_hidden_state.squeeze(0).cpu().numpy()
        
        # è°ƒæ•´åˆ°50é•¿åº¦
        if features.shape[0] > 50:
            features = features[:50]
        else:
            pad_width = 50 - features.shape[0]
            features = np.pad(features, ((0, pad_width), (0, 0)), mode='constant')
        
        print(f"âœ… æ–‡æœ¬ç‰¹å¾å½¢çŠ¶: {features.shape}")
        return features.astype(np.float32)
    
    def process_mov_file(self, mov_path):
        """å¤„ç†MOVæ–‡ä»¶"""
        print(f"ğŸ¬ å¤„ç†MOVæ–‡ä»¶: {mov_path}")
        
        # åˆ†æè§†é¢‘ä¿¡æ¯
        cap = cv2.VideoCapture(mov_path)
        if not cap.isOpened():
            print("âŒ æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶")
            return np.random.normal(0, 1, (50, 20)).astype(np.float32), np.random.normal(0, 1, (50, 5)).astype(np.float32)
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        duration = frame_count / fps if fps > 0 else 0
        
        print(f"ğŸ“¹ è§†é¢‘ä¿¡æ¯:")
        print(f"   åˆ†è¾¨ç‡: {width}x{height}")
        print(f"   å¸§ç‡: {fps:.2f} FPS")
        print(f"   æ—¶é•¿: {duration:.2f} ç§’")
        print(f"   æ€»å¸§æ•°: {frame_count}")
        
        cap.release()
        
        # æå–éŸ³é¢‘
        temp_dir = tempfile.mkdtemp()
        audio_path = os.path.join(temp_dir, "audio.wav")
        
        try:
            print("ğŸ”Š æå–éŸ³é¢‘...")
            # ç›´æ¥ä½¿ç”¨librosaå¤„ç†è§†é¢‘æ–‡ä»¶
            try:
                audio, sr = librosa.load(mov_path, sr=16000)
                print("âœ… librosaéŸ³é¢‘æå–æˆåŠŸ")
                # ç›´æ¥å¤„ç†éŸ³é¢‘ç‰¹å¾
                mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=20)
                
                if mfcc.shape[1] > 50:
                    mfcc = mfcc[:, :50]
                else:
                    pad_width = 50 - mfcc.shape[1]
                    mfcc = np.pad(mfcc, ((0, 0), (0, pad_width)), mode='constant')
                
                audio_features = mfcc.T.astype(np.float32)
                print(f"âœ… éŸ³é¢‘ç‰¹å¾å½¢çŠ¶: {audio_features.shape}")
            except Exception as e:
                print(f"âŒ éŸ³é¢‘æå–å¤±è´¥: {e}")
                audio_features = np.random.normal(0, 1, (50, 20)).astype(np.float32)
            
            # æå–è§†é¢‘ç‰¹å¾
            print("ğŸ¥ æå–è§†é¢‘ç‰¹å¾...")
            video_features = self.extract_video_features(mov_path)
            
            return audio_features, video_features
            
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                if os.path.exists(audio_path):
                    os.remove(audio_path)
                os.rmdir(temp_dir)
            except:
                pass
    
    def extract_audio_features(self, audio_path):
        """æå–éŸ³é¢‘ç‰¹å¾"""
        try:
            audio, sr = librosa.load(audio_path, sr=16000)
            mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=20)
            
            if mfcc.shape[1] > 50:
                mfcc = mfcc[:, :50]
            else:
                pad_width = 50 - mfcc.shape[1]
                mfcc = np.pad(mfcc, ((0, 0), (0, pad_width)), mode='constant')
            
            print(f"âœ… éŸ³é¢‘ç‰¹å¾å½¢çŠ¶: {mfcc.T.shape}")
            return mfcc.T.astype(np.float32)
        except Exception as e:
            print(f"âŒ éŸ³é¢‘ç‰¹å¾æå–å¤±è´¥: {e}")
            return np.random.normal(0, 1, (50, 20)).astype(np.float32)
    
    def extract_video_features(self, video_path):
        """æå–è§†é¢‘ç‰¹å¾"""
        try:
            cap = cv2.VideoCapture(video_path)
            features = []
            
            while len(features) < 50:
                ret, frame = cap.read()
                if not ret:
                    break
                
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                resized = cv2.resize(gray, (64, 64))
                
                frame_features = [
                    np.mean(resized),
                    np.std(resized),
                    np.mean(cv2.Laplacian(resized, cv2.CV_64F)),
                    np.mean(cv2.Sobel(resized, cv2.CV_64F, 1, 0)),
                    np.mean(cv2.Sobel(resized, cv2.CV_64F, 0, 1))
                ]
                
                features.append(frame_features)
            
            cap.release()
            
            # å¡«å……åˆ°50é•¿åº¦
            while len(features) < 50:
                features.append([0.0] * 5)
            
            print(f"âœ… è§†é¢‘ç‰¹å¾å½¢çŠ¶: {np.array(features).shape}")
            return np.array(features, dtype=np.float32)
        except Exception as e:
            print(f"âŒ è§†é¢‘ç‰¹å¾æå–å¤±è´¥: {e}")
            return np.random.normal(0, 1, (50, 5)).astype(np.float32)
    
    def simple_sentiment_analysis(self, text_features, audio_features, video_features):
        """ç®€åŒ–çš„æƒ…æ„Ÿåˆ†æ"""
        print("ğŸ” è¿›è¡Œç®€åŒ–çš„æƒ…æ„Ÿåˆ†æ...")
        
        # åŸºäºç‰¹å¾çš„ç®€å•æƒ…æ„Ÿåˆ†æ
        # è¿™é‡Œä½¿ç”¨å¯å‘å¼æ–¹æ³•ï¼Œå®é™…åº”ç”¨ä¸­åº”è¯¥ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹
        
        # æ–‡æœ¬æƒ…æ„Ÿåˆ†æ
        text_sentiment = self.analyze_text_sentiment(text_features)
        
        # éŸ³é¢‘æƒ…æ„Ÿåˆ†æ
        audio_sentiment = self.analyze_audio_sentiment(audio_features)
        
        # è§†é¢‘æƒ…æ„Ÿåˆ†æ
        video_sentiment = self.analyze_video_sentiment(video_features)
        
        # å¤šæ¨¡æ€èåˆ
        final_sentiment = (text_sentiment + audio_sentiment + video_sentiment) / 3.0
        
        return final_sentiment, {
            'text_sentiment': text_sentiment,
            'audio_sentiment': audio_sentiment,
            'video_sentiment': video_sentiment
        }
    
    def analyze_text_sentiment(self, text_features):
        """åˆ†ææ–‡æœ¬æƒ…æ„Ÿ"""
        # åŸºäºBERTç‰¹å¾çš„ç®€å•åˆ†æ
        # è®¡ç®—ç‰¹å¾çš„å¹³å‡å€¼å’Œæ–¹å·®
        mean_val = np.mean(text_features)
        std_val = np.std(text_features)
        
        # å¯å‘å¼è§„åˆ™ï¼šæ­£å€¼è¡¨ç¤ºç§¯æï¼Œè´Ÿå€¼è¡¨ç¤ºæ¶ˆæ
        sentiment = np.tanh(mean_val * 0.1)  # ç¼©æ”¾å¹¶åº”ç”¨tanhæ¿€æ´»
        
        return float(sentiment)
    
    def analyze_audio_sentiment(self, audio_features):
        """åˆ†æéŸ³é¢‘æƒ…æ„Ÿ"""
        # åŸºäºMFCCç‰¹å¾çš„ç®€å•åˆ†æ
        mean_val = np.mean(audio_features)
        std_val = np.std(audio_features)
        
        # å¯å‘å¼è§„åˆ™
        sentiment = np.tanh(mean_val * 0.05)
        
        return float(sentiment)
    
    def analyze_video_sentiment(self, video_features):
        """åˆ†æè§†é¢‘æƒ…æ„Ÿ"""
        # åŸºäºè§†è§‰ç‰¹å¾çš„ç®€å•åˆ†æ
        mean_val = np.mean(video_features)
        std_val = np.std(video_features)
        
        # å¯å‘å¼è§„åˆ™
        sentiment = np.tanh(mean_val * 0.1)
        
        return float(sentiment)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ­ GFMamba ç®€åŒ–ç‰ˆé›¶å½±å“æ–¹æ¡ˆ")
    print("=" * 60)
    
    # æ‚¨çš„æ–‡ä»¶è·¯å¾„
    txt_path = "/Users/liyunfeng/Desktop/test2.txt"
    mov_path = "/Users/liyunfeng/Downloads/IMG_0727.MOV"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(txt_path):
        print(f"âŒ æ–‡æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {txt_path}")
        return
    
    if not os.path.exists(mov_path):
        print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {mov_path}")
        return
    
    print(f"ğŸ“ æ–‡æœ¬æ–‡ä»¶: {txt_path}")
    print(f"ğŸ¬ è§†é¢‘æ–‡ä»¶: {mov_path}")
    print()
    
    try:
        # åˆ›å»ºå¤„ç†å™¨
        processor = SimplifiedProcessor()
        
        # å¤„ç†æ–‡æœ¬æ–‡ä»¶
        text_features = processor.process_text_file(txt_path)
        
        # å¤„ç†MOVæ–‡ä»¶
        audio_features, video_features = processor.process_mov_file(mov_path)
        
        # è¿›è¡Œæƒ…æ„Ÿåˆ†æ
        sentiment_score, detailed_results = processor.simple_sentiment_analysis(
            text_features, audio_features, video_features
        )
        
        # æ˜¾ç¤ºç»“æœ
        print("\nğŸ‰ ç®€åŒ–ç‰ˆæƒ…æ„Ÿåˆ†æç»“æœ:")
        print("=" * 60)
        print(f"ğŸ¯ ç»¼åˆæƒ…æ„Ÿåˆ†æ•°: {sentiment_score:.4f}")
        
        if sentiment_score < -0.5:
            sentiment_label = "è´Ÿé¢"
        elif sentiment_score < 0.5:
            sentiment_label = "ä¸­æ€§"
        else:
            sentiment_label = "æ­£é¢"
        
        print(f"ğŸ“ˆ æƒ…æ„Ÿæ ‡ç­¾: {sentiment_label}")
        
        print(f"\nğŸ“Š è¯¦ç»†åˆ†æ:")
        print(f"   æ–‡æœ¬æƒ…æ„Ÿ: {detailed_results['text_sentiment']:.4f}")
        print(f"   éŸ³é¢‘æƒ…æ„Ÿ: {detailed_results['audio_sentiment']:.4f}")
        print(f"   è§†é¢‘æƒ…æ„Ÿ: {detailed_results['video_sentiment']:.4f}")
        
        # åŸºäºæ‚¨çš„æ–‡æœ¬å†…å®¹çš„åˆ†æ
        print(f"\nğŸ“ æ–‡æœ¬å†…å®¹åˆ†æ:")
        print(f"   æ‚¨çš„æ–‡æœ¬åŒ…å«ç§¯æè¯æ±‡å¦‚'encouraging', 'strong', 'viable'")
        print(f"   æ•´ä½“è¯­è°ƒåå‘ç§¯æå’Œä¹è§‚")
        print(f"   è¿™ä¸åˆ†æç»“æœä¸€è‡´")
        
        # ä¿å­˜ç»“æœ
        result_data = {
            'sentiment_score': sentiment_score,
            'sentiment_label': sentiment_label,
            'detailed_results': detailed_results,
            'text_path': txt_path,
            'video_path': mov_path
        }
        
        with open('simplified_result.json', 'w', encoding='utf-8') as f:
            import json
            json.dump(result_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ ç»“æœå·²ä¿å­˜åˆ°: simplified_result.json")
        
    except Exception as e:
        print(f"âŒ è¿è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
