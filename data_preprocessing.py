#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®é¢„å¤„ç†è„šæœ¬
ç”¨äºå¤„ç†å¤šæ¨¡æ€æ•°æ®ï¼Œæ”¯æŒCMU-MOSIç­‰æ•°æ®é›†æ ¼å¼
"""

import os
import pickle
import numpy as np
import librosa
import cv2
import torch
from torch.utils.data import Dataset, DataLoader
import yaml
import warnings
warnings.filterwarnings("ignore")

class MultimodalDataset(Dataset):
    """å¤šæ¨¡æ€æ•°æ®é›†ç±»"""
    
    def __init__(self, data_path, config):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            config: é…ç½®å­—å…¸
        """
        self.config = config
        self.data = self.load_data(data_path)
        
    def load_data(self, data_path):
        """åŠ è½½æ•°æ®"""
        if data_path.endswith('.pkl'):
            with open(data_path, 'rb') as f:
                data = pickle.load(f)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼: {data_path}")
        
        return data
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        """è·å–å•ä¸ªæ ·æœ¬"""
        sample = self.data[idx]
        
        # æå–å„æ¨¡æ€æ•°æ®
        text = sample.get('text', np.zeros((50, 768)))
        audio = sample.get('audio', np.zeros((50, 20)))
        video = sample.get('vision', np.zeros((50, 5)))
        labels = sample.get('labels', {'M': 0.0})
        
        return {
            'text': torch.FloatTensor(text),
            'audio': torch.FloatTensor(audio),
            'vision': torch.FloatTensor(video),
            'labels': {'M': torch.FloatTensor([labels['M']])}
        }

class DataPreprocessor:
    """æ•°æ®é¢„å¤„ç†å™¨"""
    
    def __init__(self, config_path='configs/mosi_train.yaml'):
        """åˆå§‹åŒ–é¢„å¤„ç†å™¨"""
        with open(config_path, 'r') as f:
            self.config = yaml.load(f, Loader=yaml.FullLoader)
    
    def preprocess_audio(self, audio_path, target_length=50):
        """
        é¢„å¤„ç†éŸ³é¢‘æ•°æ®
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            target_length: ç›®æ ‡åºåˆ—é•¿åº¦
            
        Returns:
            np.ndarray: é¢„å¤„ç†åçš„éŸ³é¢‘ç‰¹å¾
        """
        try:
            # åŠ è½½éŸ³é¢‘
            audio, sr = librosa.load(audio_path, sr=16000)
            
            # æå–MFCCç‰¹å¾
            mfcc = librosa.feature.mfcc(
                y=audio, 
                sr=sr, 
                n_mfcc=20,
                n_fft=2048,
                hop_length=512
            )
            
            # è°ƒæ•´åˆ°ç›®æ ‡é•¿åº¦
            if mfcc.shape[1] > target_length:
                mfcc = mfcc[:, :target_length]
            else:
                pad_width = target_length - mfcc.shape[1]
                mfcc = np.pad(mfcc, ((0, 0), (0, pad_width)), mode='constant')
            
            return mfcc.T  # [target_length, 20]
            
        except Exception as e:
            print(f"éŸ³é¢‘é¢„å¤„ç†å¤±è´¥: {e}")
            return np.zeros((target_length, 20))
    
    def preprocess_video(self, video_path, target_length=50):
        """
        é¢„å¤„ç†è§†é¢‘æ•°æ®
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            target_length: ç›®æ ‡åºåˆ—é•¿åº¦
            
        Returns:
            np.ndarray: é¢„å¤„ç†åçš„è§†é¢‘ç‰¹å¾
        """
        try:
            cap = cv2.VideoCapture(video_path)
            frames = []
            
            # æå–å¸§
            while len(frames) < target_length:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # è½¬æ¢ä¸ºç°åº¦å›¾å¹¶è°ƒæ•´å¤§å°
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                resized = cv2.resize(gray, (64, 64))
                
                # æå–è§†è§‰ç‰¹å¾
                features = [
                    np.mean(resized),  # å¹³å‡äº®åº¦
                    np.std(resized),   # äº®åº¦æ ‡å‡†å·®
                    np.mean(cv2.Laplacian(resized, cv2.CV_64F)),  # è¾¹ç¼˜å¼ºåº¦
                    np.mean(cv2.Sobel(resized, cv2.CV_64F, 1, 0)),  # æ°´å¹³æ¢¯åº¦
                    np.mean(cv2.Sobel(resized, cv2.CV_64F, 0, 1))   # å‚ç›´æ¢¯åº¦
                ]
                
                frames.append(features)
            
            cap.release()
            
            # å¡«å……åˆ°ç›®æ ‡é•¿åº¦
            while len(frames) < target_length:
                frames.append([0.0] * 5)
            
            return np.array(frames)  # [target_length, 5]
            
        except Exception as e:
            print(f"è§†é¢‘é¢„å¤„ç†å¤±è´¥: {e}")
            return np.zeros((target_length, 5))
    
    def preprocess_text(self, text, max_length=50):
        """
        é¢„å¤„ç†æ–‡æœ¬æ•°æ®
        
        Args:
            text: æ–‡æœ¬å­—ç¬¦ä¸²
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
            
        Returns:
            np.ndarray: é¢„å¤„ç†åçš„æ–‡æœ¬ç‰¹å¾
        """
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”ç”¨ä¸­éœ€è¦ä½¿ç”¨BERTç­‰é¢„è®­ç»ƒæ¨¡å‹
        # å°†æ–‡æœ¬è½¬æ¢ä¸ºå›ºå®šé•¿åº¦çš„ç‰¹å¾å‘é‡
        words = text.split()
        features = []
        
        for word in words[:max_length]:
            # ç®€å•çš„è¯åµŒå…¥ï¼ˆå®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼‰
            word_features = np.random.randn(768)  # 768ç»´ç‰¹å¾
            features.append(word_features)
        
        # å¡«å……åˆ°ç›®æ ‡é•¿åº¦
        while len(features) < max_length:
            features.append(np.zeros(768))
        
        return np.array(features)  # [max_length, 768]
    
    def create_sample_data(self, text, audio_path=None, video_path=None, label=0.0):
        """
        åˆ›å»ºæ ·æœ¬æ•°æ®
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            label: æ ‡ç­¾
            
        Returns:
            dict: æ ·æœ¬æ•°æ®å­—å…¸
        """
        sample = {
            'text': self.preprocess_text(text),
            'labels': {'M': label}
        }
        
        if audio_path and os.path.exists(audio_path):
            sample['audio'] = self.preprocess_audio(audio_path)
        else:
            sample['audio'] = np.zeros((50, 20))
        
        if video_path and os.path.exists(video_path):
            sample['vision'] = self.preprocess_video(video_path)
        else:
            sample['vision'] = np.zeros((50, 5))
        
        return sample
    
    def save_processed_data(self, data, output_path):
        """
        ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®
        
        Args:
            data: æ•°æ®åˆ—è¡¨
            output_path: è¾“å‡ºè·¯å¾„
        """
        with open(output_path, 'wb') as f:
            pickle.dump(data, f)
        print(f"æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")

def create_data_loader(data_path, config, batch_size=16, shuffle=True):
    """
    åˆ›å»ºæ•°æ®åŠ è½½å™¨
    
    Args:
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        config: é…ç½®å­—å…¸
        batch_size: æ‰¹æ¬¡å¤§å°
        shuffle: æ˜¯å¦æ‰“ä¹±æ•°æ®
        
    Returns:
        DataLoader: æ•°æ®åŠ è½½å™¨
    """
    dataset = MultimodalDataset(data_path, config)
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=4,
        pin_memory=True
    )
    return dataloader

def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºæµ‹è¯•æ•°æ®é¢„å¤„ç†"""
    print("ğŸ”§ æ•°æ®é¢„å¤„ç†æµ‹è¯•")
    print("=" * 50)
    
    # åˆå§‹åŒ–é¢„å¤„ç†å™¨
    preprocessor = DataPreprocessor()
    
    # æµ‹è¯•æ–‡æœ¬é¢„å¤„ç†
    test_text = "This is a test sentence for preprocessing."
    text_features = preprocessor.preprocess_text(test_text)
    print(f"æ–‡æœ¬ç‰¹å¾å½¢çŠ¶: {text_features.shape}")
    
    # æµ‹è¯•éŸ³é¢‘é¢„å¤„ç†ï¼ˆå¦‚æœæœ‰éŸ³é¢‘æ–‡ä»¶ï¼‰
    test_audio = "/Users/liyunfeng/Desktop/IMG_0721.MOV"  # ä»è§†é¢‘ä¸­æå–éŸ³é¢‘
    if os.path.exists(test_audio):
        try:
            audio_features = preprocessor.preprocess_audio(test_audio)
            print(f"éŸ³é¢‘ç‰¹å¾å½¢çŠ¶: {audio_features.shape}")
        except Exception as e:
            print(f"éŸ³é¢‘é¢„å¤„ç†å¤±è´¥: {e}")
    
    # æµ‹è¯•è§†é¢‘é¢„å¤„ç†
    test_video = "/Users/liyunfeng/Desktop/IMG_0721.MOV"
    if os.path.exists(test_video):
        try:
            video_features = preprocessor.preprocess_video(test_video)
            print(f"è§†é¢‘ç‰¹å¾å½¢çŠ¶: {video_features.shape}")
        except Exception as e:
            print(f"è§†é¢‘é¢„å¤„ç†å¤±è´¥: {e}")
    
    # åˆ›å»ºæ ·æœ¬æ•°æ®
    sample = preprocessor.create_sample_data(
        text=test_text,
        audio_path=test_audio if os.path.exists(test_audio) else None,
        video_path=test_video if os.path.exists(test_video) else None,
        label=0.5
    )
    
    print(f"æ ·æœ¬æ•°æ®é”®: {sample.keys()}")
    print(f"æ–‡æœ¬ç‰¹å¾å½¢çŠ¶: {sample['text'].shape}")
    print(f"éŸ³é¢‘ç‰¹å¾å½¢çŠ¶: {sample['audio'].shape}")
    print(f"è§†é¢‘ç‰¹å¾å½¢çŠ¶: {sample['vision'].shape}")
    print(f"æ ‡ç­¾: {sample['labels']['M']}")

if __name__ == "__main__":
    main()
