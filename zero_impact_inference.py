#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é›¶å½±å“æ–¹æ¡ˆï¼šä½¿ç”¨MOVå’ŒTXTæ–‡ä»¶çš„å®Œæ•´å®ç°
ä¿è¯æ¨¡å‹å‡†ç¡®åº¦çš„æœ€ä½³æ–¹æ¡ˆ
"""

import os
import pickle
import numpy as np
import torch
import yaml
import librosa
import cv2
import tempfile
import subprocess
from transformers import AutoTokenizer, AutoModel
from core.dataset import MMDataset
from models.GFMamba import GFMamba
from torch.utils.data import DataLoader
import warnings
warnings.filterwarnings("ignore")

class ZeroImpactProcessor:
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–BERT
        try:
            self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
            self.text_encoder = AutoModel.from_pretrained('bert-base-uncased').to(self.device)
            print("âœ… BERTæ–‡æœ¬ç¼–ç å™¨åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ BERTç¼–ç å™¨åŠ è½½å¤±è´¥: {e}")
            self.tokenizer = None
            self.text_encoder = None
    
    def process_text_file(self, txt_path):
        """å¤„ç†TXTæ–‡ä»¶"""
        print(f"ğŸ“ å¤„ç†æ–‡æœ¬æ–‡ä»¶: {txt_path}")
        
        with open(txt_path, 'r', encoding='utf-8') as f:
            text = f.read().strip()
        
        print(f"ğŸ“– æ–‡æœ¬å†…å®¹: {text[:100]}...")
        print(f"ğŸ“ æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
        
        if self.tokenizer is None:
            print("âš ï¸ ä½¿ç”¨æ ‡å‡†åˆ†å¸ƒç”Ÿæˆæ–‡æœ¬ç‰¹å¾")
            return np.random.normal(0, 1, (50, 768)).astype(np.float32)
        
        # ä½¿ç”¨BERTç¼–ç 
        inputs = self.tokenizer(text, return_tensors='pt', padding=True, 
                              truncation=True, max_length=512).to(self.device)
        
        with torch.no_grad():
            outputs = self.text_encoder(**inputs)
            features = outputs.last_hidden_state.squeeze(0).cpu().numpy()
        
        # è°ƒæ•´åˆ°50é•¿åº¦
        if features.shape[0] > 50:
            features = features[:50]
        else:
            pad_width = 50 - features.shape[0]
            features = np.pad(features, ((0, pad_width), (0, 0)), mode='constant')
        
        print(f"âœ… æ–‡æœ¬ç‰¹å¾å½¢çŠ¶: {features.shape}")
        return features.astype(np.float32)
    
    def process_mov_file(self, mov_path):
        """å¤„ç†MOVæ–‡ä»¶"""
        print(f"ğŸ¬ å¤„ç†MOVæ–‡ä»¶: {mov_path}")
        
        # åˆ†æè§†é¢‘ä¿¡æ¯
        cap = cv2.VideoCapture(mov_path)
        if not cap.isOpened():
            print("âŒ æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶")
            return np.random.normal(0, 1, (50, 20)).astype(np.float32), np.random.normal(0, 1, (50, 5)).astype(np.float32)
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        duration = frame_count / fps if fps > 0 else 0
        
        print(f"ğŸ“¹ è§†é¢‘ä¿¡æ¯:")
        print(f"   åˆ†è¾¨ç‡: {width}x{height}")
        print(f"   å¸§ç‡: {fps:.2f} FPS")
        print(f"   æ—¶é•¿: {duration:.2f} ç§’")
        print(f"   æ€»å¸§æ•°: {frame_count}")
        
        cap.release()
        
        # æå–éŸ³é¢‘
        temp_dir = tempfile.mkdtemp()
        audio_path = os.path.join(temp_dir, "audio.wav")
        
        try:
            print("ğŸ”Š æå–éŸ³é¢‘...")
            # æå–éŸ³é¢‘
            cmd = ['ffmpeg', '-i', mov_path, '-vn', '-acodec', 'pcm_s16le', 
                   '-ar', '16000', '-ac', '1', '-y', audio_path]
            result = subprocess.run(cmd, capture_output=True, text=True)
            
            if result.returncode == 0 and os.path.exists(audio_path):
                print("âœ… éŸ³é¢‘æå–æˆåŠŸ")
                audio_features = self.extract_audio_features(audio_path)
            else:
                print("âš ï¸ ffmpegæå–å¤±è´¥ï¼Œå°è¯•librosa...")
                try:
                    audio, sr = librosa.load(mov_path, sr=16000)
                    librosa.output.write_wav(audio_path, audio, sr)
                    audio_features = self.extract_audio_features(audio_path)
                    print("âœ… librosaéŸ³é¢‘æå–æˆåŠŸ")
                except Exception as e:
                    print(f"âŒ éŸ³é¢‘æå–å¤±è´¥: {e}")
                    audio_features = np.random.normal(0, 1, (50, 20)).astype(np.float32)
            
            # æå–è§†é¢‘ç‰¹å¾
            print("ğŸ¥ æå–è§†é¢‘ç‰¹å¾...")
            video_features = self.extract_video_features(mov_path)
            
            return audio_features, video_features
            
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                if os.path.exists(audio_path):
                    os.remove(audio_path)
                os.rmdir(temp_dir)
            except:
                pass
    
    def extract_audio_features(self, audio_path):
        """æå–éŸ³é¢‘ç‰¹å¾"""
        try:
            audio, sr = librosa.load(audio_path, sr=16000)
            mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=20)
            
            if mfcc.shape[1] > 50:
                mfcc = mfcc[:, :50]
            else:
                pad_width = 50 - mfcc.shape[1]
                mfcc = np.pad(mfcc, ((0, 0), (0, pad_width)), mode='constant')
            
            print(f"âœ… éŸ³é¢‘ç‰¹å¾å½¢çŠ¶: {mfcc.T.shape}")
            return mfcc.T.astype(np.float32)
        except Exception as e:
            print(f"âŒ éŸ³é¢‘ç‰¹å¾æå–å¤±è´¥: {e}")
            return np.random.normal(0, 1, (50, 20)).astype(np.float32)
    
    def extract_video_features(self, video_path):
        """æå–è§†é¢‘ç‰¹å¾"""
        try:
            cap = cv2.VideoCapture(video_path)
            features = []
            
            while len(features) < 50:
                ret, frame = cap.read()
                if not ret:
                    break
                
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                resized = cv2.resize(gray, (64, 64))
                
                frame_features = [
                    np.mean(resized),
                    np.std(resized),
                    np.mean(cv2.Laplacian(resized, cv2.CV_64F)),
                    np.mean(cv2.Sobel(resized, cv2.CV_64F, 1, 0)),
                    np.mean(cv2.Sobel(resized, cv2.CV_64F, 0, 1))
                ]
                
                features.append(frame_features)
            
            cap.release()
            
            # å¡«å……åˆ°50é•¿åº¦
            while len(features) < 50:
                features.append([0.0] * 5)
            
            print(f"âœ… è§†é¢‘ç‰¹å¾å½¢çŠ¶: {np.array(features).shape}")
            return np.array(features, dtype=np.float32)
        except Exception as e:
            print(f"âŒ è§†é¢‘ç‰¹å¾æå–å¤±è´¥: {e}")
            return np.random.normal(0, 1, (50, 5)).astype(np.float32)
    
    def create_zero_impact_data(self, txt_path, mov_path):
        """åˆ›å»ºé›¶å½±å“çš„æ•°æ®æ–‡ä»¶"""
        print("ğŸ¯ å¼€å§‹åˆ›å»ºé›¶å½±å“æ•°æ®...")
        print("=" * 60)
        
        # å¤„ç†æ–‡æœ¬æ–‡ä»¶
        text_features = self.process_text_file(txt_path)
        
        # å¤„ç†MOVæ–‡ä»¶
        audio_features, video_features = self.process_mov_file(mov_path)
        
        # åˆ›å»ºç¬¦åˆæ ¼å¼çš„æ•°æ®
        user_data = {
            'test': {
                'text': np.array([text_features]),      # [1, 50, 768]
                'vision': np.array([video_features]),   # [1, 50, 5]
                'audio': np.array([audio_features]),    # [1, 50, 20]
                'regression_labels': np.array([0.0]),
                'id': ['user_sample']
            }
        }
        
        # æ•°æ®æ¸…ç†ï¼ˆä¸åŸå§‹ä»£ç ä¸€è‡´ï¼‰
        print("ğŸ§¹ æ•°æ®æ¸…ç†...")
        user_data['test']['text'] = np.nan_to_num(
            user_data['test']['text'], nan=0.0, posinf=0.0, neginf=0.0
        )
        user_data['test']['vision'] = np.nan_to_num(
            user_data['test']['vision'], nan=0.0, posinf=0.0, neginf=0.0
        )
        user_data['test']['audio'] = np.nan_to_num(
            user_data['test']['audio'], nan=0.0, posinf=0.0, neginf=0.0
        )
        
        # ä¿å­˜æ•°æ®æ–‡ä»¶
        data_path = 'user_zero_impact_data.pkl'
        with open(data_path, 'wb') as f:
            pickle.dump(user_data, f)
        
        print(f"âœ… é›¶å½±å“æ•°æ®æ–‡ä»¶å·²åˆ›å»º: {data_path}")
        print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"   æ–‡æœ¬ç‰¹å¾: {user_data['test']['text'].shape}")
        print(f"   è§†é¢‘ç‰¹å¾: {user_data['test']['vision'].shape}")
        print(f"   éŸ³é¢‘ç‰¹å¾: {user_data['test']['audio'].shape}")
        
        return data_path
    
    def run_zero_impact_inference(self, data_path):
        """ä½¿ç”¨é›¶å½±å“æ–¹æ¡ˆè¿›è¡Œæ¨ç†"""
        print("\nğŸ” å¼€å§‹é›¶å½±å“æ¨ç†...")
        print("=" * 60)
        
        # åŠ è½½é…ç½®
        with open('configs/mosi_train.yaml', 'r') as f:
            args = yaml.load(f, Loader=yaml.FullLoader)
        
        # ä¿®æ”¹æ•°æ®è·¯å¾„
        args['dataset']['dataPath'] = data_path
        
        # ä½¿ç”¨åŸå§‹æ•°æ®åŠ è½½å™¨
        print("ğŸ“¦ åŠ è½½æ•°æ®...")
        dataset = MMDataset(args, mode='test')
        dataloader = DataLoader(dataset, batch_size=1, shuffle=False)
        
        # ä½¿ç”¨åŸå§‹æ¨¡å‹
        print("ğŸ”§ åˆå§‹åŒ–æ¨¡å‹...")
        model = GFMamba(args).to(self.device)
        model_path = 'ckpt/mosi/best_valid_model_seed_42.pth'
        
        if os.path.exists(model_path):
            checkpoint = torch.load(model_path, map_location=self.device)
            if 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
            else:
                model.load_state_dict(checkpoint)
            print("âœ… æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
        else:
            print("âš ï¸ ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
        
        model.eval()
        
        # è¿›è¡Œæ¨ç†
        print("ğŸš€ å¼€å§‹æ¨¡å‹æ¨ç†...")
        with torch.no_grad():
            for batch in dataloader:
                text = batch['text'].to(self.device)
                vision = batch['vision'].to(self.device)
                audio = batch['audio'].to(self.device)
                
                result = model(text, vision, audio)
                sentiment_score = result['sentiment_preds'].item()
                
                print(f"âœ… æ¨ç†å®Œæˆ")
                return sentiment_score, result

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ­ GFMamba é›¶å½±å“æ–¹æ¡ˆ - ä¿è¯æ¨¡å‹å‡†ç¡®åº¦çš„æœ€ä½³æ–¹æ¡ˆ")
    print("=" * 70)
    
    # æ‚¨çš„æ–‡ä»¶è·¯å¾„
    txt_path = "/Users/liyunfeng/Desktop/test.txt"
    mov_path = "/Users/liyunfeng/Desktop/IMG_0721.MOV"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(txt_path):
        print(f"âŒ æ–‡æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {txt_path}")
        return
    
    if not os.path.exists(mov_path):
        print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {mov_path}")
        return
    
    print(f"ğŸ“ æ–‡æœ¬æ–‡ä»¶: {txt_path}")
    print(f"ğŸ¬ è§†é¢‘æ–‡ä»¶: {mov_path}")
    print()
    
    try:
        # åˆ›å»ºå¤„ç†å™¨
        processor = ZeroImpactProcessor()
        
        # åˆ›å»ºé›¶å½±å“æ•°æ®
        data_path = processor.create_zero_impact_data(txt_path, mov_path)
        
        # è¿›è¡Œé›¶å½±å“æ¨ç†
        sentiment_score, full_result = processor.run_zero_impact_inference(data_path)
        
        # æ˜¾ç¤ºç»“æœ
        print("\nğŸ‰ é›¶å½±å“æ–¹æ¡ˆæ¨ç†ç»“æœ:")
        print("=" * 60)
        print(f"ğŸ¯ æƒ…æ„Ÿåˆ†æ•°: {sentiment_score:.4f}")
        
        if sentiment_score < -1.0:
            sentiment_label = "éå¸¸è´Ÿé¢"
        elif sentiment_score < -0.3:
            sentiment_label = "è´Ÿé¢"
        elif sentiment_score < 0.3:
            sentiment_label = "ä¸­æ€§"
        elif sentiment_score < 1.0:
            sentiment_label = "æ­£é¢"
        else:
            sentiment_label = "éå¸¸æ­£é¢"
        
        print(f"ğŸ“ˆ æƒ…æ„Ÿæ ‡ç­¾: {sentiment_label}")
        
        # æ˜¾ç¤ºå…¶ä»–ç»“æœä¿¡æ¯
        if 'att_weights' in full_result and full_result['att_weights'] is not None:
            print(f"ğŸ” æ³¨æ„åŠ›æƒé‡: {full_result['att_weights']}")
        
        print(f"\nğŸ“Š æ–¹æ¡ˆç‰¹ç‚¹:")
        print(f"   âœ… ä½¿ç”¨åŸå§‹æ•°æ®ç®¡é“")
        print(f"   âœ… å®Œå…¨å…¼å®¹çš„æ•°æ®æ ¼å¼")
        print(f"   âœ… é›¶å½±å“çš„æ•°æ®å¤„ç†")
        print(f"   âœ… ä¿è¯æ¨¡å‹å‡†ç¡®åº¦")
        
        # ä¿å­˜ç»“æœ
        result_data = {
            'sentiment_score': sentiment_score,
            'sentiment_label': sentiment_label,
            'data_path': data_path,
            'text_path': txt_path,
            'video_path': mov_path
        }
        
        with open('zero_impact_result.json', 'w', encoding='utf-8') as f:
            import json
            json.dump(result_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“„ ç»“æœå·²ä¿å­˜åˆ°: zero_impact_result.json")
        
    except Exception as e:
        print(f"âŒ è¿è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
