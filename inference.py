#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GFMamba æ¨ç†è„šæœ¬
åŸºäºREADMEä¸­çš„æ¨¡å‹æ¶æ„è¿›è¡Œå¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ææ¨ç†
"""

import os
import sys
import torch
import yaml
import numpy as np
import librosa
import cv2
from PIL import Image
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel
import warnings
warnings.filterwarnings("ignore")

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from models.GFMamba import GFMamba

class GFMambaInference:
    def __init__(self, config_path='configs/mosi_train.yaml', model_path=None):
        """
        åˆå§‹åŒ–GFMambaæ¨ç†å™¨
        
        Args:
            config_path: æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„
            model_path: é¢„è®­ç»ƒæ¨¡å‹æƒé‡è·¯å¾„
        """
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.load(f, Loader=yaml.FullLoader)
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = GFMamba(self.config).to(self.device)
        
        # åŠ è½½é¢„è®­ç»ƒæƒé‡
        if model_path and os.path.exists(model_path):
            checkpoint = torch.load(model_path, map_location=self.device)
            if 'model_state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['model_state_dict'])
            else:
                self.model.load_state_dict(checkpoint)
            print(f"æˆåŠŸåŠ è½½æ¨¡å‹æƒé‡: {model_path}")
        else:
            print("è­¦å‘Š: æœªæ‰¾åˆ°é¢„è®­ç»ƒæ¨¡å‹æƒé‡ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹")
        
        self.model.eval()
        
        # åˆå§‹åŒ–æ–‡æœ¬ç¼–ç å™¨
        try:
            self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
            self.text_encoder = AutoModel.from_pretrained('bert-base-uncased').to(self.device)
            print("æˆåŠŸåŠ è½½BERTæ–‡æœ¬ç¼–ç å™¨")
        except Exception as e:
            print(f"è­¦å‘Š: æ— æ³•åŠ è½½BERTç¼–ç å™¨: {e}")
            self.tokenizer = None
            self.text_encoder = None
    
    def preprocess_text(self, text):
        """
        é¢„å¤„ç†æ–‡æœ¬è¾“å…¥
        
        Args:
            text: è¾“å…¥æ–‡æœ¬å­—ç¬¦ä¸²
            
        Returns:
            torch.Tensor: æ–‡æœ¬ç‰¹å¾ [1, seq_len, 768]
        """
        if self.tokenizer is None or self.text_encoder is None:
            # å¦‚æœæ²¡æœ‰BERTï¼Œä½¿ç”¨ç®€å•çš„embedding
            # è¿™é‡Œç®€åŒ–ä¸ºéšæœºç‰¹å¾ï¼Œå®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¥½çš„æ–‡æœ¬ç¼–ç 
            seq_len = min(len(text.split()), 50)
            text_features = torch.randn(1, seq_len, 768).to(self.device)
            return text_features
        
        # ä½¿ç”¨BERTç¼–ç 
        inputs = self.tokenizer(
            text, 
            return_tensors='pt', 
            padding=True, 
            truncation=True, 
            max_length=512
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.text_encoder(**inputs)
            text_features = outputs.last_hidden_state  # [1, seq_len, 768]
        
        return text_features
    
    def preprocess_audio(self, audio_path, target_length=50):
        """
        é¢„å¤„ç†éŸ³é¢‘è¾“å…¥
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            target_length: ç›®æ ‡åºåˆ—é•¿åº¦
            
        Returns:
            torch.Tensor: éŸ³é¢‘ç‰¹å¾ [1, target_length, 20]
        """
        try:
            # åŠ è½½éŸ³é¢‘
            audio, sr = librosa.load(audio_path, sr=16000)
            
            # æå–MFCCç‰¹å¾
            mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=20)
            
            # è°ƒæ•´åˆ°ç›®æ ‡é•¿åº¦
            if mfcc.shape[1] > target_length:
                # æˆªæ–­
                mfcc = mfcc[:, :target_length]
            else:
                # å¡«å……
                pad_width = target_length - mfcc.shape[1]
                mfcc = np.pad(mfcc, ((0, 0), (0, pad_width)), mode='constant')
            
            # è½¬æ¢ä¸ºtensor
            audio_features = torch.FloatTensor(mfcc.T).unsqueeze(0).to(self.device)  # [1, target_length, 20]
            
            return audio_features
        except Exception as e:
            print(f"éŸ³é¢‘å¤„ç†å¤±è´¥: {e}")
            # è¿”å›é›¶å¡«å……
            return torch.zeros(1, target_length, 20).to(self.device)
    
    def preprocess_video(self, video_path, target_length=50):
        """
        é¢„å¤„ç†è§†é¢‘è¾“å…¥
        
        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            target_length: ç›®æ ‡åºåˆ—é•¿åº¦
            
        Returns:
            torch.Tensor: è§†é¢‘ç‰¹å¾ [1, target_length, 5]
        """
        try:
            cap = cv2.VideoCapture(video_path)
            frames = []
            
            # æå–å¸§
            while len(frames) < target_length:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # è½¬æ¢ä¸ºç°åº¦å›¾å¹¶è°ƒæ•´å¤§å°
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                resized = cv2.resize(gray, (64, 64))
                
                # æå–ç®€å•çš„è§†è§‰ç‰¹å¾
                features = [
                    np.mean(resized),  # å¹³å‡äº®åº¦
                    np.std(resized),   # äº®åº¦æ ‡å‡†å·®
                    np.mean(cv2.Laplacian(resized, cv2.CV_64F)),  # è¾¹ç¼˜å¼ºåº¦
                    np.mean(cv2.Sobel(resized, cv2.CV_64F, 1, 0)),  # æ°´å¹³æ¢¯åº¦
                    np.mean(cv2.Sobel(resized, cv2.CV_64F, 0, 1))   # å‚ç›´æ¢¯åº¦
                ]
                
                frames.append(features)
            
            cap.release()
            
            # å¡«å……åˆ°ç›®æ ‡é•¿åº¦
            while len(frames) < target_length:
                frames.append([0.0] * 5)
            
            # è½¬æ¢ä¸ºtensor
            video_features = torch.FloatTensor(frames).unsqueeze(0).to(self.device)  # [1, target_length, 5]
            
            return video_features
        except Exception as e:
            print(f"è§†é¢‘å¤„ç†å¤±è´¥: {e}")
            # è¿”å›é›¶å¡«å……
            return torch.zeros(1, target_length, 5).to(self.device)
    
    def predict_sentiment(self, text=None, audio_path=None, video_path=None):
        """
        é¢„æµ‹æƒ…æ„Ÿåˆ†æ•°
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            dict: åŒ…å«é¢„æµ‹ç»“æœçš„å­—å…¸
        """
        features = []
        
        # å¤„ç†æ–‡æœ¬æ¨¡æ€
        if text:
            text_features = self.preprocess_text(text)
            features.append(text_features)
        else:
            # å¦‚æœæ²¡æœ‰æ–‡æœ¬ï¼Œåˆ›å»ºé›¶å¡«å……
            text_features = torch.zeros(1, 50, 768).to(self.device)
            features.append(text_features)
        
        # å¤„ç†éŸ³é¢‘æ¨¡æ€
        if audio_path and os.path.exists(audio_path):
            audio_features = self.preprocess_audio(audio_path)
            features.append(audio_features)
        else:
            # å¦‚æœæ²¡æœ‰éŸ³é¢‘ï¼Œåˆ›å»ºé›¶å¡«å……
            audio_features = torch.zeros(1, 50, 20).to(self.device)
            features.append(audio_features)
        
        # å¤„ç†è§†é¢‘æ¨¡æ€
        if video_path and os.path.exists(video_path):
            video_features = self.preprocess_video(video_path)
            features.append(video_features)
        else:
            # å¦‚æœæ²¡æœ‰è§†é¢‘ï¼Œåˆ›å»ºé›¶å¡«å……
            video_features = torch.zeros(1, 50, 5).to(self.device)
            features.append(video_features)
        
        # æ¨¡å‹æ¨ç†
        with torch.no_grad():
            output = self.model(*features)
            sentiment_score = output['sentiment_preds'].item()
        
        # æƒ…æ„Ÿè§£é‡Š
        if sentiment_score < -1.0:
            sentiment_label = "éå¸¸è´Ÿé¢"
        elif sentiment_score < -0.3:
            sentiment_label = "è´Ÿé¢"
        elif sentiment_score < 0.3:
            sentiment_label = "ä¸­æ€§"
        elif sentiment_score < 1.0:
            sentiment_label = "æ­£é¢"
        else:
            sentiment_label = "éå¸¸æ­£é¢"
        
        return {
            'sentiment_score': sentiment_score,
            'sentiment_label': sentiment_label,
            'modalities_used': {
                'text': text is not None,
                'audio': audio_path is not None and os.path.exists(audio_path),
                'video': video_path is not None and os.path.exists(video_path)
            },
            'attention_weights': output.get('att_weights', None)
        }

def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºæµ‹è¯•"""
    print("ğŸ­ GFMamba å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ææ¨ç†")
    print("=" * 50)
    
    # åˆå§‹åŒ–æ¨ç†å™¨
    try:
        inference = GFMambaInference(
            config_path='configs/mosi_train.yaml',
            model_path='ckpt/mosi/best_valid_model_seed_42.pth'
        )
        print("âœ… æ¨ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # ç¤ºä¾‹æµ‹è¯•
    print("\nğŸ“ å¼€å§‹æµ‹è¯•...")
    
    # æµ‹è¯•æ–‡æœ¬è¾“å…¥
    test_text = "This is a great movie, I really enjoyed it!"
    result = inference.predict_sentiment(text=test_text)
    
    print(f"è¾“å…¥æ–‡æœ¬: {test_text}")
    print(f"æƒ…æ„Ÿåˆ†æ•°: {result['sentiment_score']:.3f}")
    print(f"æƒ…æ„Ÿæ ‡ç­¾: {result['sentiment_label']}")
    print(f"ä½¿ç”¨çš„æ¨¡æ€: {result['modalities_used']}")
    
    # å¦‚æœæœ‰ç”¨æˆ·çš„æ–‡ä»¶ï¼Œä¹Ÿå¯ä»¥æµ‹è¯•
    user_video = "/Users/liyunfeng/Desktop/IMG_0721.MOV"
    user_text = "/Users/liyunfeng/Desktop/test.txt"
    
    if os.path.exists(user_video) and os.path.exists(user_text):
        print(f"\nğŸ¬ æµ‹è¯•ç”¨æˆ·æ–‡ä»¶...")
        
        # è¯»å–æ–‡æœ¬å†…å®¹
        try:
            with open(user_text, 'r', encoding='utf-8') as f:
                user_text_content = f.read().strip()
            
            result = inference.predict_sentiment(
                text=user_text_content,
                video_path=user_video
            )
            
            print(f"ç”¨æˆ·æ–‡æœ¬: {user_text_content[:100]}...")
            print(f"æƒ…æ„Ÿåˆ†æ•°: {result['sentiment_score']:.3f}")
            print(f"æƒ…æ„Ÿæ ‡ç­¾: {result['sentiment_label']}")
            print(f"ä½¿ç”¨çš„æ¨¡æ€: {result['modalities_used']}")
            
        except Exception as e:
            print(f"å¤„ç†ç”¨æˆ·æ–‡ä»¶æ—¶å‡ºé”™: {e}")

if __name__ == "__main__":
    main()
