# ğŸ­ GFMamba Deployment Guides

## ğŸ“‹ Project Overview

GFMamba is a multimodal sentiment analysis model based on the Mamba architecture that integrates text, audio, and visual features for sentiment prediction. This project supports datasets such as CMU-MOSI and can be used for regression-based sentiment analysis tasks.

## ğŸ§  Model architecture

**GFMamba** The model contains the following core components:
- **ModalityProjector**: Projects features from different modalities to a unified dimension
- **ContextExtractor**: Extracts contextual information from each modality
- **TGMamba**: A cross-modal fusion module based on Mamba
- **IntraModalEnhance**: Enhances internal features of each modality
- **Graph Fusion**: Final multimodal feature fusion

## ğŸ› ï¸ Installation Requirements

### System Requirements
- **Python**: >= 3.9
- **PyTorch**: >= 2.1.0
- **CUDA**: 11.8+ (Optionalï¼Œusing GPU to speed up)

### Installation Dependency
```bash
# Clone project
git clone <repository-url>
cd GFMamba-main

# Install dependency
pip install -r requirements.txt
```

## ğŸ“ project structure

```
GFMamba-main/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ mosi_train.yaml          # Model config file
â”œâ”€â”€ core/                        # Model main function
â”‚   â”œâ”€â”€ dataset.py              # dataset process
â”‚   â”œâ”€â”€ losses.py               # loss functions
â”‚   â”œâ”€â”€ metric.py               # evalutation metrics
â”‚   â”œâ”€â”€ optimizer.py            # optimizer
â”‚   â”œâ”€â”€ scheduler.py            # learning scheduler
â”‚   â””â”€â”€ utils.py                # utils functions
â”œâ”€â”€ models/                      # model definitions
â”‚   â”œâ”€â”€ GFMamba.py              # Main entry
â”‚   â”œâ”€â”€ enhance.py              # enhance module
â”‚   â”œâ”€â”€ gl_feature.py           # global feature
â”‚   â”œâ”€â”€ graph_fusion.py         # graph fusion
â”‚   â”œâ”€â”€ Intramodel.py           # Intra model enhancement
â”‚   â””â”€â”€ mamba/                  # Mamba related models
â”œâ”€â”€ ckpt/                       # the weight of models
â”‚   â””â”€â”€ mosi/
â”‚       â””â”€â”€ best_valid_model_seed_42.pth
â”œâ”€â”€ inference.py                # inference scripts
â”œâ”€â”€ data_preprocessing.py       # data preprocessing 
â”œâ”€â”€ app.py                      # Web API
â”œâ”€â”€ train.py                    # train script
â””â”€â”€ requirements.txt            # depenency list
```

## ğŸš€ Quick start

### 1. command line inference

```bash
# basic inference
python inference.py

# use self defined files 
python inference.py --text "Your text here" --audio "audio.wav" --video "video.mp4"
```

### 2. Web API Service

```bash
# Startup Web Service
python app.py

# Visit Web UI
# http://localhost:5000
```

### 3. Data Preprocessing

```bash
# running data pre-processing test
python data_preprocessing.py
```

## ğŸ“Š Inputs

### Text Inputs
- **format**: String
- **processing**: BERT encoding (768)
- **length of sequence**: Maximum 512 tokens

### Audio Input
- **Format**: WAV, MP3, M4A
- **Sampling rate**: 16kHz
- **Features**: MFCC (20ä¸ªç³»æ•°)
- **Length of Sequence**: 50 frames

### Video Input
- **Format**: MP4, AVI, MOV
- **Resolution**: Automatically adjust to 64x64
- **Features**: 5 visual statistical features
- **Length of Sequence**: 50 frames

## ğŸ”§ Configration

### Model Configuration (configs/mosi_train.yaml)

```yaml
model:
  input_dim: [768, 20, 5]     # [Script, audio, video]Dimension
  dim: 64                     # Model internal feature dimensions
  ContextExtractor:
    conv_kernel_size: 5
    hidden_ratio: 2
    dropout: 0.5
  TGMamba:
    num_layers: 1
    dropout: 0.5
    causal: false
    mamba_config:
      d_state: 12
      expand: 2
      d_conv: 4
      bidirectional: true
  IntraModalEnhancer:
    dropout: 0.5
  graph_fusion:
    num_classes: 1
    hidden: 16
    dropout: 0.5
    fusion_hidden: 24
```

## ğŸ“ˆ Training model

```bash
# Train with default configuration
python train.py

# Use custom configuration
python train.py --config_file your_config.yaml --seed 42
```

## ğŸŒ API Usage

### Web UI
visit `http://localhost:5000` Use a graphical interface to perform sentiment analysis.

### REST API

#### sentiment analysis
```bash
curl -X POST http://localhost:5000/analyze \
  -F "text=This is a great movie!" \
  -F "audio=@audio.wav" \
  -F "video=@video.mp4"
```

#### health check
```bash
curl http://localhost:5000/health
```

#### API info
```bash
curl http://localhost:5000/api/info
```

### response format
```json
{
  "success": true,
  "data": {
    "sentiment_score": 0.5,
    "sentiment_label": "æ­£é¢",
    "modalities_used": {
      "text": true,
      "audio": true,
      "video": false
    },
    "timestamp": "2024-01-01T12:00:00"
  }
}
```

## ğŸ¯ Usage example

### Python code example

```python
from inference import GFMambaInference

# Initialize the reasoner
inference = GFMambaInference(
    config_path='configs/mosi_train.yaml',
    model_path='ckpt/mosi/best_valid_model_seed_42.pth'
)

# sentiment analysis
result = inference.predict_sentiment(
    text="This is a wonderful day!",
    audio_path="audio.wav",
    video_path="video.mp4"
)

print(f"Emotional score: {result['sentiment_score']}")
print(f"Emotional label: {result['sentiment_label']}")
```

### Data Preprocessing Example

```python
from data_preprocessing import DataPreprocessor

# Initialize preprocessor
preprocessor = DataPreprocessor()

# Creat samplong data
sample = preprocessor.create_sample_data(
    text="Sample text",
    audio_path="audio.wav",
    video_path="video.mp4",
    label=0.5
)
```

## ğŸ” Debugging

### FAQ

1. **Model loading failed**
   ```
   Error: Unable to load model weights
  Solution: Check if there is a pre-trained model file in the ckpt/mosi/ directory
   ```

2. **CUDA out of memory**
   ```
 Error: CUDA out of memory
 Solution: Reduce batch_size or use CPU mode
   ```

3. **Dependency package conflict**
   ```
  Error: ImportError
  Solution: Recreate the virtual environment and install dependencies
   ```

### debug mode
```bash
# Enable Flask debug mode
export FLASK_DEBUG=1
python app.py
```

## ğŸ“Š Performance optimization

### GPU Speed up
```python
# Check GPU availability
import torch
print(f"CUDA availability: {torch.cuda.is_available()}")
print(f"Number of GPUs: {torch.cuda.device_count()}")
```

### Batch processing
For large amounts of data, batch processing is recommended:
```python
# Batch inference
def batch_inference(inference, data_list):
    results = []
    for data in data_list:
        result = inference.predict_sentiment(**data)
        results.append(result)
    return results
```

## ğŸ”’ security considerations

- File upload restrictions: Limit file types and sizes
- Temporary file cleaning: Automatically delete uploaded temporary files
- Data privacy: Do not store user data
- Network security: HTTPS is recommended for production environments

## ğŸ“ tech support

The origin author:
If you have any questions, please contact:
- email: zzhe232@aucklanduni.ac.nz
- project repo: [GitHub Repository]

## ğŸ“„ License

This project follows the corresponding open source license, please see the LICENSE file for details.


---

**æ³¨æ„**: æœ¬ç³»ç»Ÿä¸“ä¸ºå¤šæ¨¡æ€æƒ…æ„Ÿåˆ†æè®¾è®¡ï¼Œæ”¯æŒæ–‡æœ¬ã€éŸ³é¢‘ã€è§†é¢‘ä¸‰ç§æ¨¡æ€çš„è¾“å…¥å’Œèåˆåˆ†æã€‚
